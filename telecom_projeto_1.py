# -*- coding: utf-8 -*-
"""Telecom - Projeto 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fY-RiV9ARW4ieNZhgF7NtESwoQOS5fxp

# Business project for portfolio

A fictional telecommunications company was dedicated to making informed decisions based on historical data. In order to optimize its business, the company needed to identify which customers were likely to cancel their service after their contract expired (known as "churn"), as this information could be used to target marketing efforts towards those customers and increase retention rates. By treating data as an asset, the company could gain valuable insights and make more informed decisions, ultimately leading to increased financial gains and a competitive edge in the market.

# Importing Libraries
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import warnings

warnings.filterwarnings('ignore')

"""# Importing DataSet"""

from google.colab import drive

drive.mount('/content/drive')

df=pd.read_csv('/content/drive/My Drive/DataSet/telecom_costumer_churn.csv')

#I've used Pandas to import.

df.head()

"""# Descriptive analysis"""

import pandas as pd

data = {'Attribute': ['CustomerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn'],
        'Type': ['Categorical Nominal', 'Categorical Nominal', 'Binary Categorical', 'Binary Categorical', 'Binary Categorical', 'Discrete Numeric', 'Binary Categorical', 'Binary Categorical', 'Categorical Nominal', 'Binary Categorical', 'Binary Categorical', 'Binary Categorical', 'Binary Categorical', 'Binary Categorical', 'Binary Categorical', 'Categorical Nominal', 'Binary Categorical', 'Categorical Nominal', 'Continuous Numeric', 'Continuous Numeric', 'Binary Categorical'],
        'Description': ['- Primary key - identifier attribute',
                        "- Informs the customer's gender (Male and Female)",
                        '- Informs whether the customer is a senior citizen (1) or not (0)',
                        '- Informs whether the customer closed directly with the partner company or with a reseller partner.',
                        '- Informs whether there are other people linked to the main customer\'s contract. In practice, such data is useful for better understanding the family and social dynamics of customers, and therefore can be more efficient in providing plan and marketing solutions.',
                        '- Number of months the customer has been with the company since the first contract',
                        '- Informs whether the customer has a phone service contract.',
                        '- Informs how many phone lines were contracted.',
                        '- Informs the type of internet service contracted (Optical Fiber, DSL)',
                        '- Additional security service for those who have hired an internet network.',
                        '- Service that provides users with a system for backup, storage and retrieval of computer files.',
                        '- Device protection service (works as an insurance for cell phones).',
                        '- Technical support service.',
                        '- Cable TV service (such as Sky, etc.)',
                        '- Streaming service (such as Netflix, etc.)',
                        '- Types of contracts closed (monthly, yearly and biennial).',
                        '- Informs whether the payment method is "paperless billing", that is, whether the customer pays completely electronically, or if there is any physical resource.',
                        '- Payment method (4 available).',
                        "- Customer's monthly fee.",
                        '- Total amount of revenue derived from the customer since the beginning of the contract.',
                        '- Finally, it informs whether such a customer has churned (1), or not (0).']}

Description = pd.DataFrame(data)

display(Description)

df.describe()

df.dtypes

df['TotalCharges']= pd.to_numeric(df['TotalCharges'], errors='coerce')
df.isnull().sum()

df.corr()['TotalCharges']

"""As the "TotalCharges" variable has a high correlation with "tenure", we will use the median of customers whose service relationship time (tenure) is in a similar interval."""

df[pd.isnull(df['TotalCharges'])]

"""However, all customers with NaN in "TotalCharges" have 0 in the "tenure" attribute.

Explanation: These are customers who were recently added to the company. So, they have a monthly fee, but since the contract was closed recently, a month has not yet passed since the dataset was extracted.
"""

#So, we will exclude such rows from the DataFrame.

df=df.dropna(axis=0)

df.isnull().sum()

df['Churn'].replace(to_replace='Yes', value=1, inplace=True)
df['Churn'].replace(to_replace='No', value=0, inplace=True)

df=df.iloc[:,1:]

df.head()

#It is necessary to transform categorical variables into binary numerical variables to analyze correlation.

df2= pd.get_dummies(df)

sns.set_palette('dark')
sns.set_style('dark')

plt.figure(figsize=(15,9))

corr= df2.corr()['Churn']
mask = (corr > 0.25) | (corr < -0.25)
if mask.any():
    corr[mask].sort_values(ascending = False).plot.bar()
plt.title('Correlation in Decendent order', size=15)

df2.corr()['Churn'].sort_values(ascending=False)

"""Insight One:
The variable that had the highest correlation with Churn was 'Contract_Month-to-month', with 40% correlation, indicating that the possibility of the customer paying monthly (as opposed to annually) makes them more likely to cancel (Churn). This is supported by the fact that Contract_One year has -17.82% correlation and Contract_Two year has -30.15% correlation. The longer the proposed contract time, the lower the Churn rate. Perhaps customers who close annual contracts become more confident in the product and its efficiency, which represents less than half of the total data, since about 55% of closed contracts are 'Month to Month'.

# Individual Variable Analysis: Exploring the Behavior of Each Feature

## Demographic info
"""

plt.figure(figsize=(10,5))


order=df['gender'].value_counts()
ax=sns.countplot(data=df, x='gender', palette= 'dark', order=order.index)

plt.title('Frequency of the Genders', size=15)
patches= ax.patches
porcentagem=(df['gender'].value_counts()*100/len(df['gender']))

for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+50
   ax.annotate('{:.1f}%'.format(porcentagem[i]), (x, y), ha='center')
plt.show()

plt.figure(figsize=(10,5))

order=df['SeniorCitizen'].value_counts()
ax=sns.countplot(data=df, x='SeniorCitizen', palette= 'dark', order=order.index)

plt.title('Frequency of senior citizens in the dataset', size=15)
patches= ax.patches
porcentagem=(df['SeniorCitizen'].value_counts()*100/len(df['SeniorCitizen']))

for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+50
   ax.annotate('{:.1f}%'.format(porcentagem[i]), (x, y), ha='center')
plt.show()

plt.figure(figsize=(10,5))


order=df['Partner'].value_counts()
ax=sns.countplot(data=df, x='Partner', palette= 'dark', order=order.index, hue='Dependents')

plt.title('Frequency of contracts closed with partners and dependents', size=15)

plt.figure(figsize=(10,5))


sns.distplot(df['tenure'], hist=True, kde=False, bins=int(36), color='darkblue', hist_kws={'edgecolor':'black'})
plt.title('Amount of customers by theirtenure', size=15)
plt.xlabel('Tenure(months)')
plt.ylabel('Amount of customers')

min_ylim, max_ylim = plt.ylim()
plt.text(df['tenure'].mean()*1.05, max_ylim*0.90, 'Mean (μ): {:.2f}'.format(df['tenure'].mean()))

"""As suspected, most customers are concentrated in the first few months of their contracts, with another peak at the end.

## Services Info
"""

df.columns.values

plt.figure(figsize=(10,5))

order=df['InternetService'].value_counts()

ax=sns.countplot(data=df, x='InternetService', palette= 'dark', order=order.index)

plt.title('Frequency of internet service types', size=15)
patches= ax.patches
porcentagem=(df['InternetService'].value_counts()*100/len(df['InternetService']))


for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+5
   ax.annotate('{:.1f}%'.format(porcentagem[i]), (x, y), ha='center')
plt.show()

"""20% of the contracts do not include internet service."""

service=['PhoneService', 'MultipleLines', 'InternetService',
       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
       'TechSupport', 'StreamingTV', 'StreamingMovies']

fig, axes = plt.subplots(nrows = 3,ncols = 3,figsize = (15,12))
for i, item in enumerate(service):
    if i < 3:
         df[item].value_counts().plot.bar(ax=axes[i,0],rot = 0, title=item)
        
    elif i >=3 and i < 6:
         df[item].value_counts().plot.bar(ax=axes[i-3,1],rot = 0, title=item)
        
    elif i < 9:
         df[item].value_counts().plot.bar(ax=axes[i-6,2],rot = 0, title=item)

"""More than 90% of customers have phone service in their contract, with just over half of them using multiple lines.
On the other hand, internet service covers approximately 50% of the customers in the DataFrame. This indicates that a significant number of customers enter into contracts with the company without internet service, which may represent an opportunity as there are several other services that can only be offered to customers who purchase internet.
"""

plt.figure(figsize=(10,5))

sns.countplot(x= df['PhoneService'], hue=df['InternetService'])
plt.title('Phone Service and Internet Service Subscription Distribution', size=15)

"""Business question:

Why do all customers in the dataset who do not have phone service only use DSL internet?

Fiber optic internet is the company's main service (by frequency). Is it also the most profitable in terms of ROIC?

How do customers who subscribe to this type of service behave? Do they tend to stay longer with the company? Do they have low churn rates?

## Customer account information
"""

#Pelos gráficos abaixo, supôe-se que o serviço de fibra óptica é que possui ticket maior.

fig, ax= plt.subplots(nrows=1, ncols=3, figsize=(19,6))
fig.suptitle('Frequency distribution of monthly charges by internet services.', fontsize=15)

sns.histplot(x=df['MonthlyCharges'][df['InternetService']=='Fiber optic'], ax=ax[0])
ax[0].set_xlabel('Fiber optic')
sns.histplot(x=df['MonthlyCharges'][df['InternetService']=='DSL'], ax=ax[1])
ax[1].set_xlabel('DSL')
sns.histplot(x=df['MonthlyCharges'][df['InternetService']=='No'], ax=ax[2])
ax[2].set_xlabel('No')
print('Mean monthly charge of customers with Fiber optic service.: {:.2f}' .format(df['MonthlyCharges'][df['InternetService']=='Fiber optic'].mean()))
print('Mean monthly charge of customers with DSL.: {:.2f}' .format(df['MonthlyCharges'][df['InternetService']=='DSL'].mean()))
print('Mean monthly charge of customers with no phone service.:  {:.2f}\n\n' .format(df['MonthlyCharges'][df['InternetService']=='No'].mean()))

"""The most requested type of internet generates, on average, the highest revenue for the company."""

plt.figure(figsize=(10,5))

order=df['Contract'].value_counts()
ax=sns.countplot(data=df, x='Contract', palette= 'dark', order=order.index)

plt.title('Frequency of contract types', size=15)
porcentage=(df['Contract'].value_counts()*100/len(df['Contract']))
patches= ax.patches

for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+50
   ax.annotate('{:.1f}%'.format(porcentage[i]), (x, y), ha='center')
plt.show()

fig, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, sharey = True, figsize = (19,6))

fig.suptitle('Frequency Distribution of Tenure by Contract Type', fontsize=20)
ax1 = sns.distplot(df[df['Contract']=='Month-to-month']['tenure'], hist=True, kde=False, ax=ax1)
ax1.set_xlabel('Month-to-month')

ax2 = sns.distplot(df[df['Contract']=='One year']['tenure'], hist=True, kde=False, ax=ax2)
ax2.set_xlabel('One year')

ax3 = sns.distplot(df[df['Contract']=='Two year']['tenure'], hist=True, kde=False, ax=ax3)
ax3.set_xlabel('Two year')

plt.figure(figsize=(10,5))

order=df['PaperlessBilling'].value_counts()
ax=sns.countplot(data=df, x='PaperlessBilling', palette= 'dark', order=order.index)

plt.title('Frequency distribution of customers who pay through electronic mail', size=15)
patches= ax.patches
porcentagem=(df['PaperlessBilling'].value_counts()*100/len(df['PaperlessBilling']))

for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+50
   ax.annotate('{:.1f}%'.format(porcentagem[i]), (x, y), ha='center')
plt.show()

plt.figure(figsize=(10,5))
ax = sns.countplot(x='PaymentMethod', data=df)

plt.title('Percentage of Payment methods', size=15)
ax.set_xlabel('Payment Method')
ax.set_ylabel('Percentage of Customers')
ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])
ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha="right")

"""The payment methods are well distributed, with emphasis on eCheck."""

plt.figure(figsize=(10,6))
sns.histplot(df['MonthlyCharges'], bins=50)
plt.title('Frequency of customers according to the amount paid monthly', size=15)
plt.xlabel('Mensalidade')

df['MonthlyCharges'].median()

min_ylim, max_ylim = plt.ylim()
plt.axvline(df['MonthlyCharges'].mean(), color='red', linestyle= '--', linewidth= 0.8)
plt.text(df['MonthlyCharges'].mean()*1.05, max_ylim*0.96, 'Mean (μ): {:.2f}'.format(df['MonthlyCharges'].mean()))

"""The majority of the distribution is concentrated on a lower MonthlyCharges. However, after this initial concentration (after 23..), there seems to be a possible increase in the frequency of the MonthlyCharges.

Insight: In fact, most people will pay the basic fee for the initial service. However, by being open to accepting additional services, there is a possibility of significantly increasing the MonthlyCharges.
"""

plt.figure(figsize=(10,6))
sns.histplot(df['TotalCharges'], bins=50)
plt.title('Frequency of customers according to the total amount paid', size=15)
plt.xlabel('Total')

"""There are long-standing customers who have brought significant revenue to the company due to their length of stay."""

plt.figure(figsize=(15,6))
df.plot.scatter(x='MonthlyCharges' , y='TotalCharges')
plt.title('Relationship between the total amount paid by the customer and their current monthlyCharges', size=15)

"""The key to earning for the company is through increasing the monthlyCharge per customer. This is logical, but what the graph shows us is:

- Hypothesis: Customers who pay more per month tend to stay longer.
"""

plt.figure(figsize=(14,8))
df.plot.scatter(x='MonthlyCharges' , y='tenure')
plt.title('elationship between total contract time and MonthlyCharges.', size=15)

"""The correlation between the total contract time and the MonthlyCharges has a correlation equal to 0.25, which constitutes a low significant relationship.

- Since there is no strong linear relationship between the 'tenure' and 'MonthlyCharges' variables, that is, between the time spent by the customer with the company and their monthlycharges, this is a hypothesis that needs to be further investigated and analyzed.

# Examining the Relationship between Variables and Churn Rate
"""

plt.figure(figsize=(10,5))

order=df['Churn'].value_counts()
ax=sns.countplot(data=df, x='Churn', palette= 'dark', order=order.index)

plt.title('Churn Rate', size=15)
patches= ax.patches
porcentagem=(df['Churn'].value_counts()*100/len(df['Churn']))

for i in range(len(patches)):
   x = patches[i].get_x() + patches[i].get_width()/2
   y = patches[i].get_height()+50
   ax.annotate('{:.1f}%'.format(porcentagem[i]), (x, y), ha='center')
plt.show()

"""- Unbalanced target variable"""

plt.figure(figsize=(10,5))
sns.boxplot(x= df['Churn'], y= df['tenure'])

plt.title('Box plot analysis of contract duration in relation to the Churn', size=15)

"""- Tenure is the variable with the highest negative correlation with churn rate.

In fact, according to the dataset, customers who have used the company's service for a longer period of time are less likely to churn. Dissatisfaction with the service often occurs in the early months, prompting the customer to cancel the service.
"""

df2['tenure'] = pd.qcut(df['tenure'], 10)

plt.figure(figsize=(20,8))
sns.countplot(data= df2, x='tenure', hue='Churn')

plt.title('Frequency distribution of churn rate based on specific contract duration intervals', size=20)

"""It is clear from the data that the churn rate decreased considerably as the customer stayed longer with the company."""

plt.figure(figsize=(10,5))

sns.barplot(y=df['Churn'], x= df['Contract'], hue=df['TechSupport'])

plt.title('Churn Rate by Contract Type and Online Security', size=15)

"""Customers without technical support have a higher churn rate."""

contract= df.groupby(['Contract','Churn']).size().unstack()


contract.plot.bar(figsize=(10,5))
plt.title('Distribution of customers by contract type and Churn ', size=15)

"""Monthly closed contracts show a high Churn Rate.
In theory, the longer the contracts, the lower the probability of churn.
"""

sc= df.groupby(['SeniorCitizen','Churn']).size().unstack()

sc.plot.bar(figsize=(10,5))
plt.title('Frequency of Churn according to Senior Citizen status', size=15)

"""Despite seniors representing only 20% of the dataset, they have a significantly higher Churn Rate."""

plt.figure(figsize=(12,6))
sns.barplot(y=df['Churn'], x= df['PaymentMethod'])
plt.title('Churn Rate according to payment methods', size=15)
plt.ylabel('Churn')


plt.subplots_adjust(bottom=0.3, wspace=0.4)

"""Automated payments clearly have a lower Churn Rate than payments that require customer action."""

plt.figure(figsize=(12,6))
sns.barplot(y=df['Churn'], x= df['StreamingMovies'])
plt.title('Churn Rate according to payment methods', size=15)
plt.ylabel('Churn')


plt.subplots_adjust(bottom=0.3, wspace=0.4)

Partner_Dependents= df.groupby(['Partner','Dependents','Churn']).size().unstack()

Partner_Dependents.plot.bar(figsize=(10,5))
plt.title('Churn Rate according to Partner relationship and Dependents', size=15)
plt.xticks(rotation=45)

"""The ChurnRate is lower when the customer has "partner" and "dependent"."""

plt.figure(figsize=(12,6))

sns.kdeplot(data=df[df['Churn']==1], x='MonthlyCharges', shade=True, hue_order=['Month-to-month', 'One year', 'Two year'])
sns.kdeplot(data=df[df['Churn']==0], x='MonthlyCharges', shade=True, hue_order=['Month-to-month', 'One year', 'Two year'])

plt.legend(labels=["Yes - Churn","No - Churn"])
plt.title('Density Probability Plot of Monthly Charges Paid by Customers Based on Whether They Churned or Not', size=15)

min_ylim, max_ylim = plt.ylim()
plt.axvline(df['MonthlyCharges'][df['Churn']==1].mean(), color='blue', linestyle= '--', linewidth= 0.8)
plt.text(df['MonthlyCharges'][df['Churn']==1].mean()*1.05, max_ylim*0.96, 'Mean (μ): {:.2f}'.format(df['MonthlyCharges'][df['Churn']==1].mean()))

plt.axvline(df['MonthlyCharges'][df['Churn']==0].mean(), color='red', linestyle= '--', linewidth= 0.8)
plt.text(df['MonthlyCharges'][df['Churn']==0].mean()*0.88, max_ylim*0.96, 'Mean (μ): {:.2f}'.format(df['MonthlyCharges'][df['Churn']==0].mean()))

"""Customers with higher monthly charges tend to have a higher churn rate."""

contracts = df['Contract'].unique()


fig, axes = plt.subplots(ncols=1, nrows=len(contracts), figsize=(7,20), sharey=True)


for i, contract in enumerate(contracts):
    sns.kdeplot(df.loc[(df['Contract'] == contract) & (df['Churn'] == 1), 'MonthlyCharges'], shade=True, ax=axes[i], label="Deu Churn")
    sns.kdeplot(df.loc[(df['Contract'] == contract) & (df['Churn'] == 0), 'MonthlyCharges'], shade=True, ax=axes[i], label="Não deu Churn")
    
  
    axes[i].set_title(f'Contrato: {contract}', fontsize=15)
    
   
    axes[i].axvline(df.loc[(df['Contract'] == contract) & (df['Churn'] == 1), 'MonthlyCharges'].mean(), color='blue', linestyle='--', linewidth=0.8)
    axes[i].axvline(df.loc[(df['Contract'] == contract) & (df['Churn'] == 0), 'MonthlyCharges'].mean(), color='red', linestyle='--', linewidth=0.8)

    axes[i].legend()
    axes[i].set_xlabel('MonthlyCharges')
    

fig.suptitle('Density Probability Plot of Monthly Charges paid by customers according to Churn in each type of contract.', size=15)

fig.tight_layout(pad=2)
plt.subplots_adjust(top=0.85)

"""The observed behavior remains independent of the type of contract.

A média da mensalidade dos clientes que deram Churn é aproximadamente 20% maior.
Hipótese: Num cenário macroeconômico instável, a população tende a reduzir as despesas que mais pesam no bolso.
"""

plt.figure(figsize=(12,6))

sns.kdeplot(df['TotalCharges'][df['Churn']==1], shade=True)
sns.kdeplot(df['TotalCharges'][df['Churn']==0], shade=True)

plt.legend(labels=["Deu Churn","Não deu Churn"])
plt.title('Density Probability Graph of the amount paid by customers based on whether they have churned or not', size=15)

"""# Data Preparation

The data preparation process is crucial because the quality of features used to train a machine learning model is critical to its performance. Feature engineering involves creating, selecting, and transforming features from raw data to improve data representation for the model and, consequently, improve model performance in the target task.

By observing our data, we noticed that there is a set of additional services offered to customers who have contracted internet service, namely: OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, and StreamingMovies. Given that there is a certain level of correlation, although not necessarily strong, between these services and our target variable (Churn), we will use feature extraction to create two new variables to synthesize all services and potentially obtain better predictors.

We will consider the four services related to security and protection, namely OnlineSecurity, OnlineBackup, DeviceProtection, and TechSupport, as additional services, and create the binary variable "SecurityServices," which will receive a value of 1 if any of the four services are contracted.

On the other hand, we will create the binary variable "Streaming," which will receive a value of 1 if any of the two streaming services are subscribed.

Finally, we believe that the payment method may have an influence on the ChurnRate. Specifically, whether the customer pays automatically or not can be a good predictor. Therefore, we will create the variable 'Automatic_Payment.
"""

df['Streaming'] = np.where((df2['StreamingTV_Yes'] == 1) | (df2['StreamingMovies_Yes'] == 1), 1, 0)

df['SecurityServices'] = np.where((df2['OnlineSecurity_Yes'] == 1) | (df2['OnlineBackup_Yes'] == 1) | 
                                  (df2['DeviceProtection_Yes'] == 1) | (df2['TechSupport_Yes'] == 1), 1, 0)
df['Automatic_Payment'] = np.where((df2['PaymentMethod_Bank transfer (automatic)'] == 1) | (df2['PaymentMethod_Credit card (automatic)'] == 1), 1, 0)

plt.figure(figsize=(10,6))
sns.heatmap(df2[['OnlineSecurity_Yes', 'OnlineBackup_Yes', 'DeviceProtection_Yes','TechSupport_Yes','StreamingTV_Yes', 'StreamingMovies_Yes']].corr(), annot=True, cmap='Blues')
plt.title('Correlation between service features', size=15)
plt.xticks(rotation=45)

sc= df.groupby(['Streaming','Churn']).size().unstack()

sc.plot.bar(figsize=(10,5))
plt.title('Churn according to whether the customer has streaming or not', size=15)

sc= df.groupby(['SecurityServices','Churn']).size().unstack()

sc.plot.bar(figsize=(10,5))

plt.title('Churn according to whether the customer has security services or not', size=15)

df= pd.get_dummies(df, columns=['gender','InternetService','Contract'])

features=['Partner','Dependents','PhoneService','MultipleLines','OnlineSecurity','OnlineBackup','DeviceProtection',
          'TechSupport','StreamingTV','StreamingMovies','PaperlessBilling']
for feature in features:
    df[feature] = df[feature].astype(str)
    df[feature]= np.where((df[feature]=='Yes'), 1,0) 
    
df.drop('PaymentMethod', axis=1, inplace=True)
df.drop('gender_Female', axis=1, inplace=True)


cols = df.columns.tolist()
cols.remove('Churn')
df = pd.concat([df[cols], df['Churn']], axis=1)



df

df = df.reset_index(drop=True)

X= df.drop('Churn', axis=1)
y= df['Churn']

from sklearn.preprocessing import StandardScaler

num_columns=['tenure', 'TotalCharges', 'MonthlyCharges']
df.dtypes

SC= StandardScaler()
df[num_columns] = SC.fit_transform(df[num_columns])

df['MonthlyCharges']

"""# Hyperparameter Tuning -> Feature Selection -> Modeling

### Hyperparameter Tuning

After the feature engineering step, we need to apply some processes in order to extract the best possible accuracy from our machine learning model.

Each dataset can have its own characteristics, which means that the ideal parameters for one dataset may not be the same for another dataset. Parameter tuning allows the model to be adapted to the specific characteristics of the dataset, thus finding a balance between the complexity of the model and its generalization ability.

To perform this procedure, we choose RandomizedSearchCV.

Why not **GridSearchCV**, which tests all possible values and their relationships? Because its applicability can prove to be more operationally costly, making its neighboring counterparts, such as RandomizedSearchCV, more useful depending on the problem.

### Feature Selection
There are several algorithms that perform feature selection in a plausible and efficient way. Namely:

- PCA (dimensionality reduction)
- FeatureSelection
- SelectFromModel
- SelectKBest

We will use SelectKBest for our project because it is a consistent and effective algorithm. We will create a loop to test several values for the hyperparameter ''K'' (number of features to be selected), and plot a graph to determine the optimal value.

### Now, the Modeling!

Our target variable is binary. Therefore, it is a classification problem.
The appropriate choice of the model we will use depends on numerous factors. I will mention some:

Availability of labeled data
Type of problem
Dimensionality of features (attributes)
...
As we saw earlier, our target variable is unbalanced. That is, the number of customers who gave Churn is considerably less than the number who did not give Churn (about 1/3 less!).

We ran some tests and checked that Logistic Regression is suitable for this task.
I tested models more aggressively, and it produced a good accuracy.
Therefore, its application follows below.
"""

from sklearn.feature_selection import SelectKBest
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV


plt.figure(figsize=(15,7))


models=[LogisticRegression]

#Loop for choose K value.

for model in models:
    k_score=[]
    print('Modelo: {}'.format(model.__name__))
    clf = model()
    
    #Establishing hyperparameters to be selected
    param_grid = {
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'C': np.arange(1, 1000, 10),
    'max_iter': np.arange(100, 2000, 100)
}
    search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=20, cv=5, random_state=42)
    search.fit(X, y)
    
    # catching the bests hyperparams
    best_params = search.best_params_
    # applying to the model
    clf.set_params(**best_params)
    print('Melhores parâmetros:', best_params)
    
    for k in range(2, X.shape[1]+1, 2):

        selector = SelectKBest(k=k)
        X_selected = selector.fit_transform(X, y)

        
        
        #Building a pipeline with SelectKBest and model with the relevant features.
        pipeline= Pipeline([('selector', selector), 
                            ('clf',clf)]) 
        
        #Splitting the data to obtain a model applicable to reality (using Cross-Validation)
        strat_k_fold= StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
        scores = cross_val_score(pipeline, X_selected, y, cv=strat_k_fold)
        
        #Calculating the score using the mean.
        acc = np.mean(scores)
        k_score.append(acc)
        
        print('k: {}, acc {}' .format(k, acc))
        
        
    plt.plot(range(2, X.shape[1]+1, 2), k_score, label= model.__name__)

plt.xlabel('K value')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Observing the plotted graph above, we can see a concrete stabilization of accuracy from K=14. Therefore, we can configure a total quantity of around 14 attributes for a eficient application of our model."""

selector = SelectKBest(k=14)
X_selected = selector.fit_transform(X, y)
clf.fit(X_selected, y)

"""### Which were the selected features?

### SelectKBest
"""

top_k_idx = selector.scores_.argsort()[::-1][:14]

# Get the names of the top 14 features
feature_names = X.columns[top_k_idx]

# Print the names of the top 14 features
print("Top 14 features:", feature_names)

score_selector= selector.scores_
mask= selector.get_support()

pd.Series(score_selector, index= X.columns).sort_values()

"""The selectKbest algorithm calculates feature selection scores using the f_classifier method. This test calculates the variance between the target variable and each predictor variable. The resulting scores are used to select the top features for the model. The higher the score, the more relevant the feature is for the model.

### LogisticRegression Coeficients
"""

#Get the column names
features = new_X.columns

#Get the k best features
selector.fit(new_X, y)

#Get the indices of the selected columns
selected_features = selector.get_support(indices=True)

#Create a new DataFrame with the selected features from SelectKBest
selected_df = new_X.iloc[:, selected_features]

#Get the coefficients of the logistic regression
clf.fit(selected_df, y)
coefficients = clf.coef_[0]

#Get the index of the columns ordered in decreasing order
ordered_columns_index = np.argsort(coefficients)[::-1]

#Get the names of the columns corresponding to the ordered coefficients
ordered_columns_names = selected_df.columns[ordered_columns_index]

#Create a DataFrame to store the columns and their corresponding coefficients
df_coef = pd.DataFrame({'Feature': ordered_columns_names, 'Coefficient': coefficients[ordered_columns_index]})

#Display the DataFrame sorted by coefficients in decreasing order
display(df_coef)

"""print(new_X.shape[1])
print(len(selected_features))

There are several reasons why SelectKBest and logistic regression may select different sets of important variables.

One possible reason is that SelectKBest and logistic regression evaluate variables in different ways. SelectKBest may select variables with high correlation with the target variable, even if this correlation is not linear, while logistic regression may select variables with high linear correlation with the target variable. In addition, logistic regression may capture interactions between variables that SelectKBest cannot.

Another possible reason is that SelectKBest may overestimate the importance of some variables and underestimate the importance of others. This can happen if SelectKBest does not take into account the interaction between variables. For example, two variables that are not important on their own may become important together, because the interaction between them is important for explaining the target variable.

# Metrics for model evaluation

We will use a confusion matrix to metrically visualize our model. This method basically consists of dividing our predictions into 4 types through a 2x2 matrix.

- True positive: Predicted that the customer would "Churn" and they did.
- False Positive: Predicted that the customer would "Churn" and they didn't. (Type 1 error)
- True negative: Predicted that the customer would not "Churn" and they didn't.
- False negative: Predicted that the customer would not "Churn" and they did. (Type 2 error)

The crucial point where we must pay attention is the False negative, since this classification is highly costly for the business, as we will not be able to approach the customer with the purpose of trying to retain them.

Our model is useful because, by knowing which customers are more likely to "Churn", we can develop promotions and discounts, thereby increasing their chance of remaining.
"""

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import accuracy_score, confusion_matrix



confusion_matrix_accumulator= None
i=1

for train_index, test_index in strat_k_fold.split(X_selected, y):
    clf.fit(X_selected[train_index], y[train_index])
    ypred = clf.predict(X_selected[test_index])
    cm=confusion_matrix(y[test_index], ypred)
    print( print('n_kfold: {} - ConfusionMatrix: \n{}' .format(i, cm))
    )
    
    if confusion_matrix_accumulator is None:
        confusion_matrix_accumulator = cm
    else:
        confusion_matrix_accumulator += cm
    i=i+1
        
confusion_matrix_mean = confusion_matrix_accumulator / strat_k_fold.n_splits
print('Segue a média:')
print(confusion_matrix_mean)

sns.heatmap(confusion_matrix_mean , annot=True, fmt='.1f', cmap='Blues')
plt.xlabel('Previsão')
plt.ylabel('Verdadeiro')
plt.title('Confusion Matrix')
plt.show()

"""We will use an additional metric to evaluate our model.

The ROC curve shows the performance of the classification algorithm for each threshold point, graphically relating two variables: True Positive Rate (Recall or Sensitivity) and False Positive Rate.

We are facing a well-known trade-off: how to choose a good threshold to keep the model accurate, but most importantly, to maintain a high sensitivity. In practice, we want to effectively classify all customers who have churned (increasing the True Positive Rate) and thereby reducing the number of False Negatives.

To do this, we need to lower the probability threshold, making our filter less stringent in classifying a customer as having churned. However, this reduction leads to a decrease in precision, as more customers will be classified as having churned, leading to occasional imprecision in the classification.
"""

TP= confusion_matrix_mean[1,1]

FP= confusion_matrix_mean[0,1]

TN= confusion_matrix_mean[0,0]

FN= confusion_matrix_mean[1,0]

"""Analogously to the procedure performed in generating the confusion matrix, we will generate the mean Roc_auc_score value based on each Cross_validation split."""

roc_auc_accumulator= None
i=1

for train_index, test_index in strat_k_fold.split(X_selected, y):
    clf.fit(X_selected[train_index], y[train_index])
    ypred = clf.predict_proba(X_selected[test_index])
    roc_auc=roc_auc_score(y[test_index], ypred[:,1])
    print('n_kfold: {} - roc_auc: {}' .format(i, roc_auc))
    
    if roc_auc_accumulator is None:
        roc_auc_accumulator = roc_auc
      
    else:
        roc_auc_accumulator += roc_auc
    i=i+1
        
roc_auc_mean = roc_auc_accumulator / strat_k_fold.n_splits
print('Segue a média:')
print(roc_auc_mean)

from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42, shuffle=True)

clf.fit(x_train, y_train)
ypred= clf.predict_proba(x_test)[:,1]
lr_fpr, lr_tpr, _ = roc_curve(y_test, ypred)

plt.figure(figsize=(15,9))
plt.plot(lr_fpr, lr_tpr, marker='.', label= 'Logistic')

plt.title('Roc Curve', size=15)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.show()

"""We will loop through various thresholds and select the best one based on the f1_score metric."""

from sklearn.metrics import f1_score, precision_recall_fscore_support
import numpy as np

thresholds = np.arange(0, 1, 0.01)

precisions = []
recalls = []
f1_scores = []
y_proba=cross_val_predict(clf, X_selected, y, cv=strat_k_fold, method='predict_proba')[:,1]

# calculate the precision, recall, and F1 score values for each threshold value.
for t in thresholds:
    y_pred = (y_proba >= t).astype(int)
    precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='binary')
    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)
    print('threshold: {}' .format(t))
    print('Precision: {}' .format(precision))
    print('Recall: {}' .format(recall))
    print('f1_score: {} \n' .format(f1))
 
# Find the best Threshold based on f1-score
best_threshold_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_threshold_idx]

print("The best Threshold is :", best_threshold)

"""We'll use 0.36 threshold value to construct our definitive confusionmatrix.

# In what ways can we use the analysis to drive financial gains for the company"

To conclude, it is essential to adopt tools that can effectively convey the insights gained from the analysis and apply them to the current business. In order to demonstrate how the technical apparatus represented by the analysis above will be used in decision-making, we will use the analytical structure:

Expected Value
This structure consists of evaluating possible outcomes and the associated gains or losses for each outcome, based on the probabilities predicted by the model. With this information, the company can make informed decisions about how to allocate resources and which actions to take to maximize gains and minimize losses.

To do so, we need to be aware of a necessary concept for our calculation:

*Total Churn Cost per customer = (Customer Lifetime - Average number of months that the customer stays until churn) * Average gross profit that the company earns per customer

Customer Lifetime: We will calculate the average of the "tenure" variable.

Average number of months that the customer stays until churn: We will calculate the average "tenure" variable only for customers who churned.

Average gross profit per customer: We will calculate the average of the monthly revenue and multiply it by the usual gross margin of the sector (47%).

By applying the Expected Value framework and using the above calculations, the company can gain valuable insights into the cost of churn per customer and make informed decisions on how to reduce this cost and increase profitability.
"""

customer_lifetime= df['tenure'].mean()
average_month_churn= df['tenure'][df['Churn']==1].mean()
gross_profit= df['MonthlyCharges'].mean() * 0.47
total_costumers= 7032

total_cost = round((customer_lifetime - average_month_churn) * gross_profit, 2)
total_cost
total_cost_discount= total_cost*0.85
total_cost_discount
discount= total_cost - total_cost_discount


costbenefit_mat= np.array([[0, -65.98],
                            [-439.95, 373.87]], dtype= float)

"""Our cost/benefit confusion_matrix will be based on the following strategy:

When classifying as TP: we will have the possibility of approaching the customer with the aim of retaining them in the company. Our strategy is to offer a 15% discount. The benefit will be: total cost of the customer - discount 15% = $373.87.

FP: We will give a discount mistakenly to a customer who would not churn, therefore, they would stay in the company anyway. The value of the loss is: $65.98.

TN: Cost/benefit will be 0, after all, we correctly predicted that the customer would already stay in the company.

FN: Our point of attention! We will lose the total cost of the customer: $439.85.
"""

sns.heatmap(costbenefit_mat , annot=True, fmt='.1f', cmap='Blues')

profits = []
thresholds = sorted(y_proba, reverse=True)

    
for T in thresholds:
  y_pred = (y_proba > T).astype(int)
  confusion_mat = confusion_matrix(y, y_pred)
        
  profit = np.sum(confusion_mat * costbenefit_mat) / len(y)
  profits.append(profit)


plt.figure(figsize= (15,9))    
plt.title('Profit Curve', size=16)
plt.xlabel('Percentage of test examples (ranked by descending score)')
max_profit = max(profits)
min_ylim, max_ylim = plt.ylim()
plt.plot(np.linspace(0, 1, len(y)), profits, label = 'LogisticRegression, max profit ${} per user'.format(max_profit))



plt.text(max_profit*1.05, max_ylim*0.96, 'Luco máximo: {:.2f}'.format(max_profit))

"""The purpose of plotting a profit curve is to help evaluate the performance of a binary classification model in terms of its profitability. The curve shows how much profit the model generates at different decision thresholds, which are determined by sorting the test examples by their predicted probability of belonging to the positive class in descending order."""

best_threshold

y_proba=cross_val_predict(clf, X_selected, y, cv=strat_k_fold, method='predict_proba')[:,1]

y_pred_best= (y_proba >= best_threshold).astype(int)
confusion_matrix_best= confusion_matrix(y, y_pred_best)
sns.heatmap(confusion_matrix_best , annot=True, fmt='.0f', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

probability_tp= confusion_matrix_best[1,1]/7032
probability_fp= confusion_matrix_best[0,1]/7032
probability_tn= confusion_matrix_best[0,0]/7032
probability_fn= confusion_matrix_best[1,0]/7032

Valor_esperado_por_cliente= ((probability_tp*total_cost_discount) + (probability_fp*(-discount)) + (probability_tn*0) + (probability_fn*total_cost))
Valor_esperado_por_cliente

"""# Recomendations

1- Incentivizing long-term contracts can be a good recommendation based on our Analysis and SelectKBest insights.
The contract type has a considerable importance to our variable Churn, indicating that customers who sign up for two-year contracts are less likely to churn.
 Therefore, encouraging customers to sign up for longer-term contracts could potentially reduce churn rate and increase customer retention. This can be done through offering attractive discounts or exclusive benefits to customers who choose longer-term contracts. Additionally, 
offering high-quality customer service and personalized support to these customers can also help to build loyalty and increase the chances of contract renewal.

2- The analysis revealed that customers who have Fiber optic InternetService are more likely to churn. Therefore, we recommend that the company should consider improving the quality of the Fiber optic service and communication with customers who have this service to address any issues or concerns they may have. Additionally, the company could consider offering incentives or promotions

3- Elderly customers have a higher churn rate compared to other age groups. Focusing on improving the customer experience and providing tailored services to meet the needs of this customer segment to increase retentio could be a valid approach.

4-Based on the logistic regression analysis, it appears that customers who do not have Techsupport and OnlineSecurity services have a higher churn rate. Therefore, a recommendation that could be made is to encourage customers to sign up for these services, this could potentially help to reduce the churn rate and improve customer retention.
To achieve this, the company should consider launching targeted marketing campaigns to highlight the importance and benefits of these services.

5-The convenience of not having to manually make payments can reduce the likelihood of missed payments and late fees, which can contribute to customer dissatisfaction and ultimately lead to churn. By promoting automatic payment options and emphasizing the benefits of consistency and convenience, companies may be able to reduce churn rates and improve customer retention.
"""